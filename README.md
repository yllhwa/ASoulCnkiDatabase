# 枝网磁盘哈希数据库设计

## 文件说明

| 文件名        | 功能               |
| ------------- | ------------------ |
| pull_data.py  | 从数据库下载小作文 |
| train_data.py | 训练小作文         |
| merge_key.py  | 合并训练数据为一个 |
| test_check.py | 测试查重           |

**可能需要提前创建文件夹**
| 文件夹 | 功能 |
| ------ | ---------------- |
| data | 存放下载的小作文 |
| key | 存放训练结果 |
| merged | 存放合并后结果 |

## 设计

以 mysql 数据库翻页为划分点，每页训练的 hash->[rpid, rpid, ...]先存储在内存(根据内存占用调整翻页大小)。  
按 hash 排序后顺序写入 db 文件，同时每个 hash 对应 value 开头的文件偏移以(hash, offset)的形式存储在 des 文件中。  
合并时类似归并排序，将多个文件顺序归并即可。  
读取时将 des 文件载入到内存中，每次读取 hash 文件指针 seek 到偏移处读取指定长度的内容到内存即可(长度可以通过下个 hash 的偏移计算出来)

## 性能

### 内存占用:

只有下载数据时未写入磁盘的数据和描述偏移的 des 的表存在内存中。  
其中下载数据的内存占用通过分页大小和同时下载的进程数量控制。  
偏移表只与 hash 数量正相关，通过 hash 模数控制。

### 磁盘占用:

db 文件每个 value 占用 6 个字节(最大可以表示 2^48 内数据)  
des 文件每个 key 占用 6 个字节, 偏移占用 6 个字节(寻址能力 256T)

### cpu 占用:

?

### 时间消耗:

测试 100w 条小作文数据，每页 1 万条小作文。

- 下载数据:59.621s
- 训练数据:28.917s
- 合并数据:3m49.670s

测试 300w 条小作文(与线上版本相当)

- 查“嘉然小姐的狗”:0.133s

测试 600w 条小作文(线上版本两倍)

- 查“嘉然小姐的狗”:0.315s

注: 查重时间复杂度为 m\*n, 即 m 个哈希对应的文章每篇都会循环一次

## 优化

### 数据库优化

- 下载中多进程未使用连接池，每次均会重新连接数据库
- 合并过程为单线程合并，可以使用多进程多次合并
- 查重过程瓶颈疑似在 python 的字典统计数量上(?)

### 算法优化

- 无关小作文过多，可以筛选去除一部分
- 哈希函数离散度不好，大部分聚集在低处
